{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1049d5bd",
   "metadata": {},
   "source": [
    "# Data Mining with Python - Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a5fb42",
   "metadata": {},
   "source": [
    "\n",
    "This notebook contains alternative examples for Chapter 1 of the book **Learning Data Mining with Python**.\n",
    "The chapter introduces basic data mining concepts, using Python for implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfafe06",
   "metadata": {},
   "source": [
    "## Example 1: Affinity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b10555",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Sample data: transactions with different values\n",
    "data = {\n",
    "    'X': [1, 0, 1, 1, 0],\n",
    "    'Y': [1, 1, 0, 0, 1],\n",
    "    'Z': [0, 1, 0, 1, 1],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply the Apriori algorithm\n",
    "frequent_itemsets = apriori(df, min_support=0.2, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "\n",
    "# Display results\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec57331",
   "metadata": {},
   "source": [
    "## Example 2: OneR Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d6daf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def one_r_train(X, y):\n",
    "    num_features = X.shape[1]\n",
    "    best_feature = -1\n",
    "    best_error = float('inf')\n",
    "    best_predictors = None\n",
    "\n",
    "    for feature_index in range(num_features):\n",
    "        feature_values = np.unique(X[:, feature_index])\n",
    "        predictors = {}\n",
    "        total_error = 0\n",
    "\n",
    "        for value in feature_values:\n",
    "            class_counts = np.bincount(y[X[:, feature_index] == value])\n",
    "            most_frequent_class = np.argmax(class_counts)\n",
    "            error = np.sum(y[X[:, feature_index] == value] != most_frequent_class)\n",
    "            predictors[value] = most_frequent_class\n",
    "            total_error += error\n",
    "\n",
    "        if total_error < best_error:\n",
    "            best_error = total_error\n",
    "            best_feature = feature_index\n",
    "            best_predictors = predictors\n",
    "\n",
    "    return best_feature, best_predictors\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 0], [0, 1], [1, 1], [0, 0], [1, 0]])\n",
    "y = np.array([0, 1, 1, 0, 1])\n",
    "\n",
    "# Split the data into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the OneR model\n",
    "best_feature, predictors = one_r_train(X_train, y_train)\n",
    "\n",
    "# Predict function\n",
    "def predict(X_test, predictors, feature_index):\n",
    "    return np.array([predictors.get(sample[feature_index], -1) for sample in X_test])\n",
    "\n",
    "# Get predictions\n",
    "y_pred = predict(X_test, predictors, best_feature)\n",
    "\n",
    "# Print accuracy\n",
    "accuracy = np.mean(y_pred == y_test) * 100\n",
    "print(\"The test accuracy is {:.1f}%\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e586715d",
   "metadata": {},
   "source": [
    "## Example 3: Training and Testing Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08d316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Classifier Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c52ee5c",
   "metadata": {},
   "source": [
    "## Example 4: Accuracy Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0337536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Since we have new accuracy results from Example 3\n",
    "# Calculate accuracy of the Random Forest model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Classifier Accuracy (Validation): {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34734ce7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This notebook demonstrated alternative data mining techniques from Chapter 1, including affinity analysis with different metrics, a revised OneR algorithm implementation, a Random Forest classifier for training and testing, and accuracy validation. All examples were implemented and evaluated using Python code in Jupyter Notebook."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
